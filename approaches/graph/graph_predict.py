import json
import os
import time
from collections import *

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

from androguard.misc import AnalyzeAPK
from approaches.graph.graph_utils import graph_utils
from approaches.graph.graph_utils.GraphObject import GraphObject
from utils import constants

logTag = 'Graph'


def getEdgesSequence(filename):
    if filename.endswith('.json'):
        with open(filename, 'r') as f:
            d = json.load(f)
            return d['edges']


def getOpcodeNgram(ops, n=constants.N_GRAM):
    opngramlist = [tuple(ops[i:i + n]) for i in range(len(ops) - n)]
    opngram = Counter(opngramlist)
    return opngram


def create_and_save_graph(path_to_apk, path_to_save):
    print '[%s]: Creating graph...' % logTag
    print '[%s]: Analyzing...' % logTag
    a, d, dx = AnalyzeAPK(path_to_apk)
    # print a.show()
    print '[%s]: Generating graph...' % logTag
    CFG, op_count = graph_utils.get_graph(a, d, dx)
    # nx.write_gml(CFG)
    print '[%s]: Operations count: %s' % (logTag, op_count)
    save_graph(CFG, path_to_save)
    print '[%s]: Saved to: %s' % (logTag, path_to_save)


def save_graph(graph, filename):
    edges = []
    nodes = []
    for edge in graph.edges:
        item = dict()
        item['source'] = edge[0]
        item['target'] = edge[1]
        edges.append(item)

    for node in graph.nodes:
        nodes.append(node)

    obj = GraphObject(edges=edges, nodes=nodes)
    with open(filename, 'w') as outfile:
        json.dump(obj.get_dict(), outfile)


def extract_op3gram(path_to_json):
    print '[%s]: Extracting ngram...' % logTag
    opcodes = getEdgesSequence(path_to_json)
    op3gram = getOpcodeNgram(opcodes)
    return op3gram


def predict(apk_file_location):
    print '[%s]: Predicting...' % logTag
    subtrainLabel = pd.read_csv(constants.CSV_SUBTRAIN_GRAPH)
    subtrainfeature = pd.read_csv(constants.CSV_FEATURES_GRAPH)
    malware_classes = pd.read_csv(constants.CSV_CLASSES)

    subtrain = pd.merge(subtrainLabel, subtrainfeature, on='Id')
    labels = subtrain.Class
    subtrain.drop(["Class", "Id"], axis=1, inplace=True)
    subtrain = subtrain.as_matrix()

    X_train, X_test, y_train, y_test = train_test_split(subtrain, labels, test_size=0.3)

    srf = RandomForestClassifier(n_estimators=500, n_jobs=-1)
    srf.fit(X_train, y_train)
    print '[%s]: Score: %s' % (logTag, srf.score(X_test, y_test))
    # y_pred = srf.predict(X_test)
    # print y_pred
    # print confusion_matrix(y_test, y_pred)

    create_and_save_graph(apk_file_location, 'res.json')
    op3grams = extract_op3gram('res.json')

    subtrainfeature = subtrainfeature.drop(["Id"], axis=1)

    print '[%s]: Counting...' % logTag
    tmp_counter = []
    for column_index in range(0, len(subtrainfeature._get_axis(1))):
        val = 0
        key = None
        for i in op3grams:
            v1 = str(subtrainfeature._get_axis(1)[column_index])
            v2 = str(i)
            key = v1
            if v1 == v2:
                val = op3grams[i]
                break
        tmp_counter.append(val)

    predicted_class = srf.predict([tmp_counter])
    prediction_probability_all = srf.predict_proba([tmp_counter])

    # Show only class number and prediction probability
    predicted_class = predicted_class.flatten()[0]
    prediction_probability = prediction_probability_all.flatten()[predicted_class - 1]

    class_name = malware_classes[malware_classes.ClassId == predicted_class].ClassName.get_values()[0]
    return class_name, prediction_probability, prediction_probability_all


if __name__ == '__main__':
    class_name, prediction_probability, prediction_probability_all = predict(
        '/home/mkr/Desktop/projects/python/databases/cic_and_mal_2017/apks/test_fam/scareware_13bca905ccc119771040fd5fd30afff1.apk')

    print '----------------------------'
    print 'Class: %s' % class_name
    print 'Probability: %s' % str(prediction_probability)
    print prediction_probability_all
    print '----------------------------'
